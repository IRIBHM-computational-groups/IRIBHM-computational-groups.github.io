{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "import requests  # to scrap papers\n",
    "import mammoth # convertion of docx to html\n",
    "\n",
    "import requests_openalex  # to scrap papers\n",
    "from linkify import linkify_names # add links when names mentionned\n",
    "from requests_openalex import get_author_works\n",
    "\n",
    "file_loader = FileSystemLoader('templates')\n",
    "env = Environment(loader=file_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the data\n",
    "people = pd.read_csv('data/people.csv').iloc[:, 1:]\n",
    "\n",
    "# replace mention of names with links\n",
    "people = linkify_names(people)\n",
    "\n",
    "# In the 7th field of data/people.csv, there is a listing of the person's background\n",
    "# below is a mapping to match a background to an emoji \n",
    "background_map = {\n",
    "    'Bioinf': 'üß¨',\n",
    "    'Biologist': 'ü¶†',\n",
    "    'Math': 'üßÆ',\n",
    "    'CS': 'üíª'\n",
    "}\n",
    "\n",
    "def parse_background(val):\n",
    "    if isinstance(val, str):\n",
    "        labels = [item.strip() for item in val.split(',')]\n",
    "        return [(label, background_map.get(label, '‚ùì')) for label in labels]\n",
    "    return []\n",
    "\n",
    "people[\"background\"] = people[\"background\"].apply(parse_background)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Author: Vincent Detours : https://openalex.org/A5009797000\n",
      "Found Author: Maxime Tarabichi : https://openalex.org/A5045412483\n",
      "Found Author: Ruben Lattuca : https://openalex.org/A5049254340\n",
      "Found Author: Valeriia Gulaia : https://openalex.org/A5007022413\n",
      "Found Author: Oier Azurmendi Senar : https://openalex.org/A5091982381\n"
     ]
    }
   ],
   "source": [
    "# Some of us have homonyms, those below do not (and have publications)\n",
    "papers = []\n",
    "authors = ['Vincent Detours', 'Maxime Tarabichi', 'Ruben Lattuca', 'Valeriia Gulaia', 'Oier Azurmendi Senar']\n",
    "for person in authors:\n",
    "    papers.append(get_author_works(person))\n",
    "    \n",
    "papers = pd.concat(papers)\n",
    "papers = papers.sort_values('cited_by_count', ascending=False) \\\n",
    "                      .drop_duplicates(subset=['title', 'person'], keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual person page generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(loader=FileSystemLoader(\"templates\"))\n",
    "template = env.get_template(\"person_template.html\")\n",
    "\n",
    "# Generate HTML files\n",
    "for _, person_row in people.iterrows():\n",
    "    abrev = person_row['abrev']\n",
    "    filename = f\"{abrev}.html\"\n",
    "    output_path = '../'+filename\n",
    "\n",
    "    # Filter publications for this person\n",
    "    person_papers = papers[papers[\"person\"] == person_row['name']].to_dict(orient='records')\n",
    "\n",
    "    # Render template\n",
    "    rendered = template.render(\n",
    "        person=person_row.to_dict(),\n",
    "        publications=person_papers\n",
    "    )\n",
    "\n",
    "    # Save HTML\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(rendered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# People listing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate people.html\n",
    "template = env.get_template('people.html')\n",
    "html_content = template.render(person=people.to_dict(orient='records'))\n",
    "\n",
    "with open('../people.html', 'w', encoding='utf-8') as f:\n",
    "    f.write(html_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homepage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate index.html\n",
    "template = env.get_template('index.html')\n",
    "html_content = template.render()\n",
    "with open('../index.html', 'w', encoding='utf-8') as f:\n",
    "    f.write(html_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate guidelines.html\n",
    "templateGuildelines = env.get_template('guidelines.html')\n",
    "with open(\"data/SAFE.docx\", \"rb\") as docx_file:\n",
    "    result = mammoth.convert_to_html(docx_file)\n",
    "    doc_html = result.value  # HTML string extracted from docx\n",
    "\n",
    "rendered_html = templateGuildelines.render(doc_html=doc_html)\n",
    "\n",
    "with open('../guidelines.html', 'w', encoding='utf-8') as f:\n",
    "    f.write(rendered_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate publications.html\n",
    "papers_sorted = papers.sort_values('cited_by_count', ascending=False)\n",
    "\n",
    "grouped = papers_sorted.groupby(['title', 'journal', 'year', 'doi'], as_index=False).agg({\n",
    "    'person': lambda authors: ', '.join(sorted(set(authors)))\n",
    "})\n",
    "\n",
    "grouped_sorted = grouped.sort_values('year', ascending=False)\n",
    "\n",
    "publications = grouped_sorted.to_dict(orient='records')\n",
    "\n",
    "template = env.get_template('publications.html')\n",
    "\n",
    "html_content = template.render(publications=publications)\n",
    "\n",
    "with open('../publications.html', 'w', encoding='utf-8') as f:\n",
    "    f.write(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugo3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
